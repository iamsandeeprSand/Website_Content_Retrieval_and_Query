{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\New folder (2)\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\New folder (2)\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [7020]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.13s/it]\n",
      "d:\\New folder (2)\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "d:\\New folder (2)\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:55980 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.96it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:56419 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.41it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:56971 - \"POST /query HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "\n",
    "import base64\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from flask import Flask, request, jsonify\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from pymilvus import connections, CollectionSchema, FieldSchema, DataType, Collection\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Web Scraper class definition\n",
    "class WebScraper:\n",
    "    def __init__(self, url, headers=None):\n",
    "        self.url = url\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_paragraphs(html_content):\n",
    "        if html_content:\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            paragraph = [p.text for p in soup.find_all('p')]\n",
    "            return paragraph\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def fetch_page(self):\n",
    "        response = requests.get(self.url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    def fetch_and_extract_p(self):\n",
    "        response = requests.get(self.url)\n",
    "        if response.status_code == 200:\n",
    "            paragraph = self.extract_paragraphs(response.text)\n",
    "            return \" \".join(paragraph)\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "# Function to split paragraphs into sentences\n",
    "def paragraph_to_sentences(paragraph):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    word_lists = [sentence.lower().split() for sentence in sentences]\n",
    "    return [sentence for sublist in word_lists for sentence in sublist]\n",
    "\n",
    "# Load sentence transformer model for embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embeddings(sentences):\n",
    "    embeddings = embedding_model.encode(sentences, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Define schema for Milvus collection\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384),\n",
    "    FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=512)\n",
    "]\n",
    "schema = CollectionSchema(fields, description=\"Text embeddings\")\n",
    "\n",
    "# Create collection\n",
    "collection_name = \"text_embedding_collection\"\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "# Create index\n",
    "index_params = {\n",
    "    \"index_type\": \"IVF_FLAT\",\n",
    "    \"params\": {\"nlist\": 100},\n",
    "    \"metric_type\": \"L2\"\n",
    "}\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "\n",
    "def store_in_milvus(sentences, embeddings):\n",
    "    # Create IDs for the sentences\n",
    "    ids = [i for i in range(len(sentences))]\n",
    "    \n",
    "    # Ensure embeddings are in the correct format\n",
    "    embeddings = embeddings.tolist()\n",
    "    \n",
    "    # Prepare entities for Milvus\n",
    "    entities = {\n",
    "        \"id\": ids,\n",
    "        \"embedding\": embeddings,\n",
    "        \"text\": sentences\n",
    "    }\n",
    "    \n",
    "    collection.insert([entities[\"id\"], entities[\"embedding\"], entities[\"text\"]])\n",
    "    collection.flush()\n",
    "\n",
    "def fetch_from_milvus(query_embedding, top_k=5):\n",
    "    collection.load()\n",
    "    search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "    results = collection.search(\n",
    "        data=[query_embedding],\n",
    "        anns_field=\"embedding\",\n",
    "        param=search_params,\n",
    "        limit=top_k,\n",
    "        expr=None\n",
    "    )\n",
    "    return results\n",
    "\n",
    "# Load GPT-2 tokenizer and model\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_answer(query, context, max_length=100):\n",
    "    inputs = gpt2_tokenizer.encode(query + \" \" + context, return_tensors=\"pt\")\n",
    "    outputs = gpt2_model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=3,  # Prevent repeating trigrams\n",
    "        repetition_penalty=2.0,  # Penalize repeated tokens more heavily\n",
    "        temperature=0.7,  # Sampling temperature\n",
    "        top_p=0.9  # Top-p (nucleus) sampling\n",
    "    )\n",
    "    return gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Apply the nest_asyncio patch\n",
    "nest_asyncio.apply()\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class LoadRequest(BaseModel):\n",
    "    url: str\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "    use_milvus: bool = True\n",
    "\n",
    "@app.post(\"/load\")\n",
    "async def load_website_content(request: LoadRequest):\n",
    "    scraper = WebScraper(request.url)\n",
    "    text_lists = scraper.fetch_and_extract_p()\n",
    "    flat_sentences = paragraph_to_sentences(text_lists)\n",
    "    embeddings = get_embeddings(flat_sentences)\n",
    "\n",
    "    # Store in Milvus\n",
    "    store_in_milvus(flat_sentences, embeddings)\n",
    "\n",
    "    return {\"message\": \"Content loaded successfully\"}\n",
    "\n",
    "@app.post(\"/query\")\n",
    "async def query_content(request: QueryRequest):\n",
    "    if request.use_milvus:\n",
    "        # Fetch from Milvus\n",
    "        query_embedding = get_embeddings([request.query])[0]\n",
    "        results = fetch_from_milvus(query_embedding)\n",
    "        context = [res.entity.get(\"text\") for res in results[0] if res.entity.get(\"text\") is not None]\n",
    "\n",
    "    # Generate answer using RAG (assuming it's asynchronous)\n",
    "    answer = generate_answer(request.query, \" \".join(context))\n",
    "\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
